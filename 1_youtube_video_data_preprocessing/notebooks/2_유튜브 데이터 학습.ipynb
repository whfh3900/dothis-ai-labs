{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ìœ íŠœë¸Œ ë¹„ë””ì˜¤ ë°ì´í„° ë¶„ì„\n",
    "\n",
    "ì´ Jupyter Notebookì€ ìœ íŠœë¸Œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.  \n",
    "ë¶„ì„ì˜ ì£¼ ëª©ì ì€ ìœ íŠœë¸Œ ë°ì´í„°ë¥¼ í†µí•´ ì—°ê´€ì–´ ë° ìœ íŠœë¸Œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ë¥¼ ìœ„í•œ NER(ëª…ëª…ëœ ê°œì²´ ì¸ì‹) ëª¨ë¸ì˜ ê°ì²´ëª… ë¶„ì„ê¸°ë¥¼ í™œìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.  \n",
    "\n",
    "ì—¬ê¸°ì„œ ì‚¬ìš©ëœ í•™ìŠµ ë°ì´í„°ëŠ” ìˆ˜ì§‘í•œ ìœ íŠœë¸Œ ë°ì´í„°ì˜ ì¼ë¶€ ìƒ˜í”Œë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.  \n",
    "ë³¸ ì½”ë“œì—ì„œëŠ” ì´ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ GLiNER_ko ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ê³ , ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.  \n",
    "\n",
    "ìš°ë¦¬ì˜ ëª©í‘œëŠ” NER ëª¨ë¸ì´ ê°ì²´ëª…ì„ ì •í™•íˆ ì¸ì‹í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ëª…ì‚¬ë¥¼ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜í–ˆëŠ”ì§€ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "## ëª…ì‚¬ë§Œ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”©í•œ ì´ìœ \n",
    "\n",
    "í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ë§Œ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”©í•˜ëŠ” ì´ìœ ëŠ” ëª…ì‚¬ê°€ ì£¼ë¡œ ë¬¸ì¥ì˜ ì£¼ìš” ì˜ë¯¸ë¥¼ ì „ë‹¬í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì¡°ì‚¬ë¥¼ ë¹„ë¡¯í•œ ë¶ˆí•„ìš”í•œ ë‹¨ì–´ë“¤ì„ ì œê±°í•¨ìœ¼ë¡œì¨ ì¤‘ìš”í•œ ì •ë³´ì— ì§‘ì¤‘í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ë¶„ë¥˜ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” íš¨ê³¼ê°€ ìˆìŠµë‹ˆë‹¤.  \n",
    "ëª…ì‚¬ë§Œì„ ì‚¬ìš©í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì´ì ì´ ìˆìŠµë‹ˆë‹¤:\n",
    "- **ì •ë³´ ë°€ë„ ì¦ê°€**: ëª…ì‚¬ëŠ” ë¬¸ë§¥ì˜ í•µì‹¬ì„ êµ¬ì„±í•˜ë¯€ë¡œ, ëª¨ë¸ì´ ì¤‘ìš”í•œ ì •ë³´ì— ì§‘ì¤‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- **ì¡ìŒ ê°ì†Œ**: ì¡°ì‚¬ë‚˜ ì ‘ì†ì‚¬ì™€ ê°™ì€ ë‹¨ì–´ë“¤ì„ ì œê±°í•˜ë©´ ëª¨ë¸ì´ ë¶ˆí•„ìš”í•œ ì •ë³´ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì„ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- **ì°¨ì›ì˜ ì¶•ì†Œ**: ë¶ˆí•„ìš”í•œ ë‹¨ì–´ë¥¼ ì¤„ì„ìœ¼ë¡œì¨ í…ìŠ¤íŠ¸ì˜ ê¸¸ì´ê°€ ì¤„ì–´ë“¤ê³ , ë” íš¨ìœ¨ì ì¸ ì„ë² ë”©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "import torch\n",
    "from gliner import GLiNERConfig, GLiNER\n",
    "from gliner.training import Trainer, TrainingArguments\n",
    "from gliner.data_processing.collator import DataCollatorWithPadding, DataCollator\n",
    "from gliner.utils import load_config_as_namespace\n",
    "from gliner.data_processing import WordsSplitter, GLiNERDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 12\n",
      "Dataset is shuffled...\n",
      "Dataset is splitted...\n"
     ]
    }
   ],
   "source": [
    "# JSON íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ ë°ì´í„°ë¥¼ ì„ì€ í›„ 90%ëŠ” í•™ìŠµìš©, 10%ëŠ” í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¶„í• \n",
    "train_path = \"../data/video_sample_train_data.json\"\n",
    "\n",
    "with open(train_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print('Dataset size:', len(data))\n",
    "\n",
    "random.shuffle(data)\n",
    "print('Dataset is shuffled...')\n",
    "\n",
    "train_dataset = data[:int(len(data)*0.9)]\n",
    "test_dataset = data[int(len(data)*0.9):]\n",
    "\n",
    "print('Dataset is splitted...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<?, ?it/s]\n",
      "c:\\Users\\csu52\\anaconda3\\envs\\csu\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:562: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ì§€ì •ëœ ëª¨ë¸ IDì™€ ìºì‹œ ë””ë ‰í† ë¦¬ì—ì„œ GLiNER ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ GPU ë˜ëŠ” CPUì—ì„œ ì‚¬ìš©í•  ì¤€ë¹„ë¥¼ í•©ë‹ˆë‹¤.\n",
    "model_id = \"taeminlee/gliner_ko\"\n",
    "cache_dir = \"../models\"\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = GLiNER.from_pretrained(model_id, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì€ ë‚®ì§€ë§Œ ì›ë˜ êµ¬í˜„ì„ ëª¨ë°©í•˜ì—¬ ì„±ëŠ¥ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ëª¨ë¸ êµ¬ì„±ì— ë§ì¶° ë°ì´í„° ìˆ˜ì§‘ê¸°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "data_collator = DataCollator(model.config, data_processor=model.data_processor, prepare_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# ì„ íƒì ìœ¼ë¡œ ëª¨ë¸ì„ ë¹ ë¥¸ í›ˆë ¨ì„ ìœ„í•´ ì§€ì •ëœ ì¥ì¹˜(GPU ë˜ëŠ” CPU)ë¡œ ì´ë™ì‹œí‚¤ê³ , ì™„ë£Œ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "model.to(device)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\csu52\\anaconda3\\envs\\csu\\lib\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [25:00<23:55,  2.87s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9878, 'grad_norm': 0.0007905024685896933, 'learning_rate': 5.555555555555557e-06, 'epoch': 250.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 500/1000 [25:00<23:55,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 107.31719970703125, 'eval_runtime': 0.0921, 'eval_samples_per_second': 21.717, 'eval_steps_per_second': 10.859, 'epoch': 250.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [48:33<00:00,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0767, 'grad_norm': 5.896107904845849e-05, 'learning_rate': 0.0, 'epoch': 500.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [48:33<00:00,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 107.05731964111328, 'eval_runtime': 0.0642, 'eval_samples_per_second': 31.158, 'eval_steps_per_second': 15.579, 'epoch': 500.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [48:42<00:00,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2922.1135, 'train_samples_per_second': 1.711, 'train_steps_per_second': 0.342, 'train_loss': 3.532232666015625, 'epoch': 500.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=3.532232666015625, metrics={'train_runtime': 2922.1135, 'train_samples_per_second': 1.711, 'train_steps_per_second': 0.342, 'total_flos': 0.0, 'train_loss': 3.532232666015625, 'epoch': 500.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í›ˆë ¨ ë°ì´í„°ì…‹ì˜ í¬ê¸°ì™€ ë°°ì¹˜ í¬ê¸°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ ì—í¬í¬ ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³ , \n",
    "# ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•˜ì—¬ ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ TrainingArguments ê°ì²´ë¥¼ ìƒì„±í•œ ë‹¤ìŒ, \n",
    "# ì´ ì¸ìë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ í¬í•¨í•œ Trainer ê°ì²´ë¥¼ ì´ˆê¸°í™”í•˜ì—¬ ëª¨ë¸ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
    "num_steps = 500\n",
    "batch_size = 8\n",
    "data_size = len(train_dataset)\n",
    "num_batches = data_size // batch_size\n",
    "num_epochs = max(1, num_steps // num_batches)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/train\",\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=0.01,\n",
    "    others_lr=1e-5,\n",
    "    others_weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\", #cosine\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    focal_loss_alpha=0.75,\n",
    "    focal_loss_gamma=2,\n",
    "    num_train_epochs=num_epochs,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps = 100,\n",
    "    save_total_limit=10,\n",
    "    dataloader_num_workers = 0,\n",
    "    use_cpu = False,\n",
    "    report_to=\"none\",\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=model.data_processor.transformer_tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json not found in C:\\Users\\csu52\\work\\dothis\\dothis-ai-labs\\1_youtube_video_data_preprocessing\\models\\train\\checkpoint-1000\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# í›ˆë ¨ëœ ëª¨ë¸ì„ ì§€ì •ëœ ì²´í¬í¬ì¸íŠ¸(1000ë²ˆì§¸ ì²´í¬í¬ì¸íŠ¸)ì—ì„œ ë¡œë“œí•˜ë©°, ì´ë•Œ í† í¬ë‚˜ì´ì €ë„ í•¨ê»˜ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "trained_model = GLiNER.from_pretrained(\"../models/train/checkpoint-1000\", load_tokenizer=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "êµ­í˜ë‹¹ => organization\n",
      "ì¥ì² ë¯¼ => person\n",
      "ì²­íƒê¸ˆì§€ë²• => CIVILIZATION\n",
      "ê¶Œìµìœ„ì›ì¥ì—ê²Œ => CIVILIZATION\n",
      "ë…¸ì¢…ë©´ì˜ì› => CIVILIZATION\n"
     ]
    }
   ],
   "source": [
    "# í›ˆë ¨ëœ GLiNER ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹° ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³ , ì˜ˆì¸¡ëœ ì—”í‹°í‹°ì™€ í•´ë‹¹ ë ˆì´ë¸”ì„ ì¶œë ¥í•©ë‹ˆë‹¤. \n",
    "# ì‚¬ìš©ëœ ë ˆì´ë¸” ëª©ë¡ì—ëŠ” ë‹¤ì–‘í•œ ê°œì¸ì •ë³´ ë° ê´€ë ¨ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì˜ˆì¸¡ ê²°ê³¼ëŠ” entities ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ë˜ë©°, \n",
    "# ê° ì—”í‹°í‹°ì˜ í…ìŠ¤íŠ¸ì™€ ë ˆì´ë¸”ì´ ì¶œë ¥ë©ë‹ˆë‹¤.\n",
    "\n",
    "text = \"\"\"\n",
    "êµ­í˜ë‹¹ ë¬¸ì í•˜ë‚˜ë¡œ íŒŒì¥! ì§€ì§€ìœ¨ í­ë½ ì˜ˆì •! ì¥ì² ë¯¼, ì²­íƒê¸ˆì§€ë²•? ê¶Œìµìœ„ì›ì¥ì—ê²Œ ë¬¼ì–´ë³´ë‹ˆ...ë…¸ì¢…ë©´ì˜ì› ê°€ë§Œë‘ì§€ ì•Šê² ë‹¤!\n",
    "\"\"\"\n",
    "\n",
    "# Labels for entity prediction\n",
    "labels = [\"artifacts\", \"person\", \"animal\", \"CIVILIZATION\", \"organization\", \\\n",
    "        \"phone number\", \"address\", \"passport number\", \"email\", \"credit card number\", \\\n",
    "        \"social security number\", \"health insurance id number\", 'Business/organization', \\\n",
    "        \"mobile phone number\", \"bank account number\", \"medication\", \"cpf\", \"driver's license number\", \\\n",
    "        \"tax identification number\", \"medical condition\", \"identity card number\", \"national id number\", \\\n",
    "        \"ip address\", \"email address\", \"iban\", \"credit card expiration date\", \"username\", \\\n",
    "        \"health insurance number\", \"student id number\", \"insurance number\", \\\n",
    "        \"flight number\", \"landline phone number\", \"blood type\", \"cvv\", \\\n",
    "        \"digital signature\", \"social media handle\", \"license plate number\", \"cnpj\", \"postal code\", \\\n",
    "        \"passport_number\", \"vehicle registration number\", \"credit card brand\", \\\n",
    "        \"fax number\", \"visa number\", \"insurance company\", \"identity document number\", \\\n",
    "        \"national health insurance number\", \"cvc\", \"birth certificate number\", \"train ticket number\", \\\n",
    "        \"passport expiration date\", \"social_security_number\", \"EVENT\", \"STUDY_FIELD\", \"LOCATION\", \\\n",
    "        \"MATERIAL\", \"PLANT\", \"TERM\", \"THEORY\", 'Analysis Requirement']\n",
    "\n",
    "# Perform entity prediction\n",
    "entities = trained_model.predict_entities(text, labels, threshold=0.5)\n",
    "\n",
    "# Display predicted entities and their labels\n",
    "for entity in entities:\n",
    "    print(entity[\"text\"], \"=>\", entity[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì •í™•ë„: 0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# JSON ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open('../data/test.json', 'r', encoding='utf-8') as file:\n",
    "    test = json.load(file)\n",
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ì™€ ì‹¤ì œ ë ˆì´ë¸” ë¹„êµ\n",
    "total_score = 0\n",
    "\n",
    "for i in tqdm(test):\n",
    "    # Perform entity prediction\n",
    "    entities = trained_model.predict_entities(i['text'], labels, threshold=0.055)\n",
    "    result = [i['text'] for i in entities]\n",
    "    # ì ìˆ˜ ê³„ì‚°: ì˜ˆì¸¡ëœ ë ˆì´ë¸”ì´ ì‹¤ì œ ë ˆì´ë¸”ì— ìˆëŠ” ê²½ìš° 1ì , ì—†ìœ¼ë©´ 0ì \n",
    "    score = sum(1 for label in i[\"labels\"] if label in result)\n",
    "    total_score += score\n",
    "\n",
    "# ì „ì²´ ì ìˆ˜ ë° ì •í™•ë„ ê³„ì‚°\n",
    "total_labels = sum(len(item[\"labels\"]) for item in test)  # ì „ì²´ ë ˆì´ë¸” ìˆ˜\n",
    "accuracy = total_score / total_labels if total_labels > 0 else 0  # ì •í™•ë„ ê³„ì‚°\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"ì •í™•ë„: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
