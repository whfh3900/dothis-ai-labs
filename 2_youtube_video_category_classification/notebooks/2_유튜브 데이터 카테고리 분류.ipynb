{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\csu52\\anaconda3\\envs\\csu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_description</th>\n",
       "      <th>video_tags</th>\n",
       "      <th>video_duration</th>\n",
       "      <th>video_published</th>\n",
       "      <th>video_category</th>\n",
       "      <th>video_info_card</th>\n",
       "      <th>video_with_ads</th>\n",
       "      <th>video_end_screen</th>\n",
       "      <th>video_cluster</th>\n",
       "      <th>crawled_date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>pre_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JCqgRM-2_GU</td>\n",
       "      <td>UCU-wl-PZYxpWeDl1OIiE_hQ</td>\n",
       "      <td>어머니와 함께 방문 한 15 세 중학생 환자분의 포경수술 영상  (포경 소매법) C...</td>\n",
       "      <td>수술 적응증 상담 : 010-9190-7575\\n카카오톡 상담 : gnjurolog...</td>\n",
       "      <td>['강남비뇨기과', '박천진', '포경', '포경수술', '귀두포피', '위생관리']</td>\n",
       "      <td>75</td>\n",
       "      <td>2024-10-06</td>\n",
       "      <td>Education</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2024-10-07 17:37:39</td>\n",
       "      <td>2024</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>어머니와 함께 15 세 중학생 환자분의 포경수술 영상 포경 소매법 Circumcis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R0pRp-3S5Yo</td>\n",
       "      <td>UCm69xuysxVgNuRpmXCiXlPw</td>\n",
       "      <td>YURIAN (유리안) - FUCKING MY CITY 2024 ver. (Ft. ...</td>\n",
       "      <td>#YURIAN #유리안 #FUCKING_MY_CITY_2024 #Rredrain #...</td>\n",
       "      <td>[]</td>\n",
       "      <td>177</td>\n",
       "      <td>2024-10-06</td>\n",
       "      <td>Music</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2024-10-07 17:37:57</td>\n",
       "      <td>2024</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>YURIAN 유리안 FUCKING MY CITY 2024 ver Ft. Rredra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RsVKZrt2DSo</td>\n",
       "      <td>UCVcLszfUBN9rTmc0A0uDRSQ</td>\n",
       "      <td>대리모 사건이 소환한 합법화 논쟁 / KBC뉴스</td>\n",
       "      <td>불임이나 난임 부부가 브로커에게 돈을 주고 출산을 의뢰한 '대리모 범죄'가 14년 ...</td>\n",
       "      <td>['kbc', 'kbc광주방송', '깨비씨', 'kbc 뉴스']</td>\n",
       "      <td>158</td>\n",
       "      <td>2024-10-06</td>\n",
       "      <td>News &amp; Politics</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2024-10-07 17:38:23</td>\n",
       "      <td>2024</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>대리모 사건이 소환한 합법화 논쟁 KBC뉴스 kbc kbc광주방송 깨비씨 kbc 뉴스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2CpgC1K7jNI</td>\n",
       "      <td>UCxCsxXsJPDMfjcXEEV09m9w</td>\n",
       "      <td>대포죽순이요????푸바오 핸펀몰카사건? 10.6 푸바오 이야기 FUBAO PANDA...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>400</td>\n",
       "      <td>2024-10-06</td>\n",
       "      <td>Travel &amp; Events</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2024-10-07 17:38:24</td>\n",
       "      <td>2024</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>대포죽순이요 푸바오 핸펀몰카사건 푸바오 이야기 FUBAO PANDA CUTE ANI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C-jOF1sgdOg</td>\n",
       "      <td>UCYCb_mX7P1xKrB5iaT8y0Wg</td>\n",
       "      <td>일본에 남겨진 한국의 문화유산【아시아의 숨은 혼, 백제를 가다 Part 2】</td>\n",
       "      <td>0:00 2012년 6월 18일 다카이다야마 고분\\n1:43 2012년 6월 25일...</td>\n",
       "      <td>['TJB', 'tjb', 'tjb대전방송', '엑스포로131', '엑스포로', '...</td>\n",
       "      <td>572</td>\n",
       "      <td>2024-10-07</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2024-10-07 17:38:24</td>\n",
       "      <td>2024</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>일본에 한국의 문화유산 아시아의 숨은 혼 백제를 가다 Part 2 TJB tjb t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                channel_id  \\\n",
       "0  JCqgRM-2_GU  UCU-wl-PZYxpWeDl1OIiE_hQ   \n",
       "1  R0pRp-3S5Yo  UCm69xuysxVgNuRpmXCiXlPw   \n",
       "2  RsVKZrt2DSo  UCVcLszfUBN9rTmc0A0uDRSQ   \n",
       "3  2CpgC1K7jNI  UCxCsxXsJPDMfjcXEEV09m9w   \n",
       "4  C-jOF1sgdOg  UCYCb_mX7P1xKrB5iaT8y0Wg   \n",
       "\n",
       "                                         video_title  \\\n",
       "0  어머니와 함께 방문 한 15 세 중학생 환자분의 포경수술 영상  (포경 소매법) C...   \n",
       "1  YURIAN (유리안) - FUCKING MY CITY 2024 ver. (Ft. ...   \n",
       "2                         대리모 사건이 소환한 합법화 논쟁 / KBC뉴스   \n",
       "3  대포죽순이요????푸바오 핸펀몰카사건? 10.6 푸바오 이야기 FUBAO PANDA...   \n",
       "4         일본에 남겨진 한국의 문화유산【아시아의 숨은 혼, 백제를 가다 Part 2】   \n",
       "\n",
       "                                   video_description  \\\n",
       "0  수술 적응증 상담 : 010-9190-7575\\n카카오톡 상담 : gnjurolog...   \n",
       "1  #YURIAN #유리안 #FUCKING_MY_CITY_2024 #Rredrain #...   \n",
       "2  불임이나 난임 부부가 브로커에게 돈을 주고 출산을 의뢰한 '대리모 범죄'가 14년 ...   \n",
       "3                                                NaN   \n",
       "4  0:00 2012년 6월 18일 다카이다야마 고분\\n1:43 2012년 6월 25일...   \n",
       "\n",
       "                                          video_tags  video_duration  \\\n",
       "0    ['강남비뇨기과', '박천진', '포경', '포경수술', '귀두포피', '위생관리']              75   \n",
       "1                                                 []             177   \n",
       "2                ['kbc', 'kbc광주방송', '깨비씨', 'kbc 뉴스']             158   \n",
       "3                                                 []             400   \n",
       "4  ['TJB', 'tjb', 'tjb대전방송', '엑스포로131', '엑스포로', '...             572   \n",
       "\n",
       "  video_published   video_category  video_info_card  video_with_ads  \\\n",
       "0      2024-10-06        Education                0               0   \n",
       "1      2024-10-06            Music                0               0   \n",
       "2      2024-10-06  News & Politics                0               0   \n",
       "3      2024-10-06  Travel & Events                0               0   \n",
       "4      2024-10-07    Entertainment                0               0   \n",
       "\n",
       "   video_end_screen  video_cluster         crawled_date  year  month  day  \\\n",
       "0                 0             -1  2024-10-07 17:37:39  2024     10    6   \n",
       "1                 0             -1  2024-10-07 17:37:57  2024     10    6   \n",
       "2                 0             -1  2024-10-07 17:38:23  2024     10    6   \n",
       "3                 0             -1  2024-10-07 17:38:24  2024     10    6   \n",
       "4                 0             -1  2024-10-07 17:38:24  2024     10    7   \n",
       "\n",
       "                                            pre_text  \n",
       "0  어머니와 함께 15 세 중학생 환자분의 포경수술 영상 포경 소매법 Circumcis...  \n",
       "1  YURIAN 유리안 FUCKING MY CITY 2024 ver Ft. Rredra...  \n",
       "2   대리모 사건이 소환한 합법화 논쟁 KBC뉴스 kbc kbc광주방송 깨비씨 kbc 뉴스   \n",
       "3  대포죽순이요 푸바오 핸펀몰카사건 푸바오 이야기 FUBAO PANDA CUTE ANI...  \n",
       "4  일본에 한국의 문화유산 아시아의 숨은 혼 백제를 가다 Part 2 TJB tjb t...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전처리된 데이터 불러오기기\n",
    "df = pd.read_csv(\"../data/video_sample_add_pre.csv\", encoding=\"utf-8-sig\", index_col=0)\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글여부 판독기\n",
    "def calculate_korean_ratio(text, ratio=.0):\n",
    "    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)' # E-mail제거\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    pattern = '(http|ftp|https)://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+' # URL제거\n",
    "    text = re.sub(pattern=pattern, repl=' ', string=text)\n",
    "    clean_text = re.sub(r\"[^\\w\\s]\", \" \", text) # 특수문자 제거\n",
    "    words = clean_text.split()\n",
    "    total_words = len(words)  # 전체 단어 개수    \n",
    "    korean_words = 0  # 한글 단어 개수\n",
    "\n",
    "    for word in words:\n",
    "        for char in word:\n",
    "            if '\\uAC00' <= char <= '\\uD7A3':\n",
    "                korean_words += 1\n",
    "    try:\n",
    "        korean_ratio = korean_words / total_words\n",
    "        if korean_ratio >= ratio:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except ZeroDivisionError as e:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ZED():\n",
    "    def __init__(self, category_df,\n",
    "                 feature_extract_model='BM-K/KoSimCSE-roberta-multitask', \n",
    "                 cache_dir='../../models/huggingface',\n",
    "                 use_cluster = None,\n",
    "                 use_cuda = True):\n",
    "        \n",
    "        # Check if category_df is a string (file path) or a DataFrame\n",
    "        if isinstance(category_df, str):\n",
    "            self.category_df = pd.read_csv(category_df, encoding=\"utf-8-sig\", index_col=0)\n",
    "        elif isinstance(category_df, pd.DataFrame):\n",
    "            self.category_df = category_df\n",
    "        else:\n",
    "            raise ValueError(\"category_df must be a file path or a pandas DataFrame\")\n",
    "\n",
    "\n",
    "        if use_cluster:\n",
    "            self.category_df = self.category_df[self.category_df.cluster.isin(use_cluster)]\n",
    "        self.category_df[\"category\"] = self.category_df['대분류']+ \" \" + self.category_df['소분류']\n",
    "        \n",
    "        self.first_category_df = self.category_df[self.category_df.status==1]\n",
    "        self.second_category_df = self.category_df[self.category_df.status==2]\n",
    "\n",
    "        category_df_for_index = self.category_df.drop_duplicates([\"category\"])\n",
    "        self.category_index = {i:(int(j) if pd.isna(k) else int(k)) for i,j,k in zip(category_df_for_index[\"category\"], category_df_for_index[\"cluster\"], category_df_for_index[\"new_cluster\"])}\n",
    "        self.category_index[\"기타\"] = 0\n",
    "\n",
    "        self.first_category = (self.first_category_df['대분류'] + \" \" + self.first_category_df['소분류']).tolist()\n",
    "        self.second_category = (self.second_category_df['대분류'] + \" \" + self.second_category_df['소분류']).tolist()\n",
    "\n",
    "        self.first_category_keyword = [i.split(\"/\") for i in self.first_category_df[\"검색키워드\"]]\n",
    "        self.second_category_keyword = [\" \".join(i.split(\"/\")) for i in self.second_category_df[\"검색키워드\"]]\n",
    "        \n",
    "        self.feature_model = AutoModel.from_pretrained(feature_extract_model, cache_dir=cache_dir) \n",
    "        self.feature_tokenizer = AutoTokenizer.from_pretrained(feature_extract_model, cache_dir=cache_dir)\n",
    "    \n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.use_cuda = False\n",
    "        if use_cuda:\n",
    "            if self.device == \"cuda\":\n",
    "                self.use_cuda = True\n",
    "                # 모델을 CUDA로 이동\n",
    "                self.feature_model.to(self.device)\n",
    "            else:\n",
    "                print(\"cuda를 지원하지 않습니다.\")\n",
    "\n",
    "        self.inputs = self.feature_tokenizer(self.second_category_keyword, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            self.inputs = {key: tensor.to(self.device) for key, tensor in self.inputs.items()}  # 모든 입력 텐서를 같은 장치로 이동합니다.\n",
    "        self.label_embedding, _ = self.feature_model(**self.inputs, return_dict=False)\n",
    "\n",
    "\n",
    "    def classification(self, text, etc_score=39):\n",
    "        if isinstance(text, str):\n",
    "\n",
    "            ### 외국어 분류\n",
    "            if not calculate_korean_ratio(text, ratio=.0):\n",
    "                return \"해외채널\", 9999, 0\n",
    "            \n",
    "            ### 선분류\n",
    "            content = text.split('[SEP]')\n",
    "            youtube_category = content[-1].strip()\n",
    "            if youtube_category in [\"Gaming\"]:\n",
    "                _content = \" \".join(content).split()\n",
    "            else:\n",
    "                _content = content[0].strip().split()\n",
    "\n",
    "            for i, keywords in enumerate(self.first_category_keyword):\n",
    "                if len(set(_content) & set(keywords)) > 0:\n",
    "                    return self.first_category[i], self.category_index[self.first_category[i]], 0\n",
    "\n",
    "            ### 후분류\n",
    "            ### 텍스트 임베딩\n",
    "            try:\n",
    "                inputs = self.feature_tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "                if self.use_cuda:\n",
    "                    inputs = {key: tensor.to(self.device) for key, tensor in inputs.items()}  # 모든 입력 텐서를 같은 장치로 이동합니다.\n",
    "                embedding = self.feature_model(**inputs, return_dict=False)\n",
    "            except ValueError as e:\n",
    "                return \"기타\", 0, 0\n",
    "            sentences_similarity = dict()\n",
    "            for i, ce in enumerate(self.label_embedding):\n",
    "                sentences_similarity[self.second_category[i]] = float(self.cal_score(embedding[0][0][0], ce[0])[0][0].item())\n",
    "            sentences_similarity = self.add_score(youtube_category, sentences_similarity)\n",
    "            max_value = max(sentences_similarity.values())  # 가장 큰 값을 찾음\n",
    "\n",
    "            ### etc_score 보다 낮으면 후분류 키워드가 제목안에 들어있으면 그걸로\n",
    "            ### 아니면 기타로\n",
    "            if max_value < etc_score:\n",
    "                _content = content[0].strip().split()\n",
    "                for i, keywords in enumerate(self.second_category_keyword):\n",
    "                    if len(set(_content) & set(keywords)) > 0:\n",
    "                        return self.second_category[i], self.category_index[self.second_category[i]], 0\n",
    "                classification = \"기타\"\n",
    "            else:\n",
    "                classification = [key for key, value in sentences_similarity.items() if value == max_value][0]  # 가장 큰 값과 일치하는 모든 키를 찾음   \n",
    "            return classification, self.category_index[classification], max_value\n",
    "        \n",
    "        if isinstance(text, list):\n",
    "            results = dict()\n",
    "            not_class_texts = list()\n",
    "            not_class_categorys = list()\n",
    "\n",
    "            for index, t in enumerate(text):\n",
    "                class_status = False\n",
    "                ### 외국어 분류\n",
    "                if not calculate_korean_ratio(t, ratio=.0):\n",
    "                    class_status = True\n",
    "                    results[index] = (\"해외채널\", 9999, 0)\n",
    "                \n",
    "                ### 선분류\n",
    "                content = t.split('[SEP]')\n",
    "                youtube_category = content[-1].strip()\n",
    "                if youtube_category in [\"Gaming\"]:\n",
    "                    _content = \" \".join(content).split()\n",
    "                else:\n",
    "                    _content = content[0].strip().split()\n",
    "\n",
    "                for i, keywords in enumerate(self.first_category_keyword):\n",
    "                    if len(set(_content) & set(keywords)) > 0:\n",
    "                        class_status = True\n",
    "                        results[index] = (self.first_category[i], self.category_index[self.first_category[i]], 0)\n",
    "                        break\n",
    "                if not class_status:\n",
    "                    results[index] = \"\"\n",
    "                    not_class_texts.append(t)\n",
    "                    not_class_categorys.append(youtube_category)\n",
    "                    \n",
    "            if len(not_class_texts) == 0:\n",
    "                return [value for value in results.values()]\n",
    "\n",
    "            inputs = self.feature_tokenizer(not_class_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            if self.use_cuda:\n",
    "                inputs = {key: tensor.to(self.device) for key, tensor in inputs.items()}  # 모든 입력 텐서를 같은 장치로 이동합니다.\n",
    "            embedding = self.feature_model(**inputs, return_dict=False)\n",
    "            \n",
    "            for i, e in enumerate(embedding[0]):\n",
    "                sentences_similarity = dict()\n",
    "                for j, ce in enumerate(self.label_embedding):\n",
    "                    sentences_similarity[self.second_category[j]] = float(self.cal_score(e[0], ce[0])[0][0].item())\n",
    "                sentences_similarity = self.add_score(not_class_categorys[i], sentences_similarity)\n",
    "                max_value = max(sentences_similarity.values())  # 가장 큰 값을 찾음\n",
    "                ### etc_score 보다 낮으면 후분류 키워드가 제목안에 들어있으면 그걸로\n",
    "                ### 아니면 기타로\n",
    "\n",
    "                if max_value < etc_score:\n",
    "                    etc_status = True\n",
    "                    _content = content[0].strip().split()\n",
    "                    for j, keywords in enumerate(self.second_category_keyword):\n",
    "                        if len(set(_content) & set(keywords)) > 0:\n",
    "                            etc_status = False\n",
    "                            for key, value in results.items():\n",
    "                                if value == \"\":\n",
    "                                    results[key] = (self.second_category[j], self.category_index[self.second_category[j]], 0)\n",
    "                                    break\n",
    "                            break\n",
    "                    if etc_status:\n",
    "                        for key, value in results.items():\n",
    "                            if value == \"\":\n",
    "                                results[key] = (\"기타\", 0, 0)\n",
    "                                break\n",
    "                else:\n",
    "                    classification = [key for key, value in sentences_similarity.items() if value == max_value][0]  # 가장 큰 값과 일치하는 모든 키를 찾음   \n",
    "                    for key, value in results.items():\n",
    "                        if value == \"\":\n",
    "                            results[key] = (classification, self.category_index[classification], max_value)\n",
    "                            break\n",
    "\n",
    "            return [value for value in results.values()]\n",
    "\n",
    "    ### 거리계산 함수\n",
    "    def cal_score(self, a, b):\n",
    "        if len(a.shape) == 1: a = a.unsqueeze(0)\n",
    "        if len(b.shape) == 1: b = b.unsqueeze(0)\n",
    "\n",
    "        a_norm = a / a.norm(dim=1)[:, None]\n",
    "        b_norm = b / b.norm(dim=1)[:, None]\n",
    "        return torch.mm(a_norm, b_norm.transpose(0, 1)) * 100 \n",
    "\n",
    "\n",
    "    ### 카테고리에 따른 추가점수\n",
    "    def add_score(self, youtube_category, sentences_similarity):\n",
    "        \n",
    "        if youtube_category == \"Music\":\n",
    "            for key in sentences_similarity:\n",
    "                first_word = key.split()[0]  # 키를 split하여 첫 번째 원소를 추출\n",
    "                if first_word == \"음악\":\n",
    "                    sentences_similarity[key] += 5  # 값에 5를 더함\n",
    "\n",
    "        elif youtube_category == \"Gaming\":\n",
    "            for key in sentences_similarity:\n",
    "                first_word = key.split()[0]  # 키를 split하여 첫 번째 원소를 추출\n",
    "                if (first_word == \"게임\"):\n",
    "                    sentences_similarity[key] += 5  # 값에 5를 더함\n",
    "\n",
    "        elif youtube_category == \"Sports\":\n",
    "            for key in sentences_similarity:\n",
    "                first_word = key.split()[0]  # 키를 split하여 첫 번째 원소를 추출\n",
    "                if first_word == \"스포츠\":\n",
    "                    sentences_similarity[key] += 5  # 값에 5를 더함\n",
    "\n",
    "        elif youtube_category == \"News & Politics\":\n",
    "            for key in sentences_similarity:\n",
    "                first_word = key.split()[0]  # 키를 split하여 첫 번째 원소를 추출\n",
    "                if first_word == \"뉴스\":\n",
    "                    sentences_similarity[key] += 5  # 값에 5를 더함\n",
    "                    \n",
    "        else:\n",
    "            for key in sentences_similarity:\n",
    "                first_word = key.split()[0]  # 키를 split하여 첫 번째 원소를 추출\n",
    "                if first_word == \"게임\":\n",
    "                    sentences_similarity[key] -= 3  # 값에 -3을 뺌\n",
    "\n",
    "        return sentences_similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
